{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Iu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 2399\n"
     ]
    }
   ],
   "source": [
    "docs_path = 'inputs/dataset_tweets_WHO.txt'\n",
    "with open(docs_path) as fp:\n",
    "    tweets = json.loads(fp.read())\n",
    "\n",
    "print(\"Number of tweets:\", len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: {'en': 2353, 'es': 19, 'in': 2, 'fr': 7, 'und': 1, 'tl': 1, 'de': 6, 'ar': 2, 'ru': 2, 'uk': 1, 'ps': 1, 'ja': 4}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "lang = {} \n",
    "for tweet in tweets:\n",
    "    if tweets[tweet]['lang'] in lang:\n",
    "        lang[tweets[tweet]['lang']] += 1\n",
    "    else:\n",
    "        lang[tweets[tweet]['lang']] = 1\n",
    "        \n",
    "print(\"Languages:\", lang)\n",
    "print(sum(lang.values()) == len(tweets)) # Check is the number of extracted languages is the same as the number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code if the package is not installed: \"pip install num2words\"\n",
    "#!pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word, stop_words):\n",
    "    \"\"\"\n",
    "    Preprocess each word of the tweet getting rid of URLs, punctuation sings and stop words\n",
    "    \n",
    "    Argument:\n",
    "    word -- string (text) to be preprocessed\n",
    "    stop_words -- list of stop words to get rid of\n",
    "    \n",
    "    Returns:\n",
    "    word - the resulting processed word. False in case we don't want that word\n",
    "    \"\"\"\n",
    "\n",
    "# Eliminate URLs\n",
    "    word = re.sub(r'http\\S+', '', word) \n",
    "\n",
    "# Eliminate ampersands\n",
    "    word = re.sub(r'&\\S+', '', word) \n",
    "\n",
    "    if not word:\n",
    "        return False\n",
    "\n",
    "# Get rid of punctuation marks except \"#\" and \"@\"\n",
    "    if word[0] == '#':\n",
    "        word = '#' + word.translate(str.maketrans('', '', string.punctuation)) \n",
    "        return word\n",
    "\n",
    "    elif word[0] == '@':\n",
    "        word = '@' + word.translate(str.maketrans('', '', string.punctuation)) \n",
    "        return word\n",
    "\n",
    "    else:\n",
    "        word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Get rid of strings like '-'\n",
    "    if len(word) <= 1 and not word.isdigit(): \n",
    "        return False\n",
    "    \n",
    "# Eliminate the stopwords \n",
    "    elif word not in stop_words: \n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "import string\n",
    "import re\n",
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the tweet text calling the process_word function, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    stop_words = set()\n",
    "    for lang in stopwords.fileids():\n",
    "         stop_words |= set(stopwords.words(lang))\n",
    "            \n",
    "    line = emoji.get_emoji_regexp().sub(\"\", line)\n",
    "    \n",
    "    line= line.lower()## Transform in lowercase\n",
    "    line= line.split() ## Tokenize the text to get a list of terms\n",
    "    templine=[]\n",
    "    for word in line:      \n",
    "        word = process_word(word, stop_words)\n",
    "        if word:\n",
    "            templine.append(word)\n",
    "            \n",
    "    line= templine\n",
    "    line= [stemmer.stem(word) for word in line] ## perform stemming\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def create_index(tweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of tweets\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of tweets where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    for tweet in tweets:\n",
    "        line = tweets[tweet]['full_text']\n",
    "        line_arr = line.replace(\"\\n\", ' ')\n",
    "        tweet_id = tweet\n",
    "        terms = build_terms(''.join(line_arr))\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_tweet_index ==> { ‘term1’: [current_tweet, [list of positions]], ...,‘term_n’: [current_tweet, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_tweet has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_tweet_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "\n",
    "        ## the term ‘web’ appears in tweet 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in tweet 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_tweet_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains the text of the tweet\n",
    "            try:\n",
    "                # if the term is already in the index for the current tweet (current_tweet_index)\n",
    "                # append the position to the corresponding list\n",
    "                \n",
    "                current_tweet_index[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_tweet_index[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        #merge the current tweet index with the main index\n",
    "        for term_page, posting_page in current_tweet_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "                         \n",
    "                    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 25.29 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # Mark the time it takes for the code to create the indexes\n",
    "index = create_index(tweets)\n",
    "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index results for the term 'researcher': []\n",
      "\n",
      "First 10 Index results for the term 'research': \n",
      "[['22', array('I', [16])], ['153', array('I', [19])], ['171', array('I', [13])], ['203', array('I', [5])], ['210', array('I', [7])], ['211', array('I', [5])], ['221', array('I', [6])], ['422', array('I', [12])], ['428', array('I', [3])], ['459', array('I', [22])]]\n"
     ]
    }
   ],
   "source": [
    "# Check the indexes by searching the desired words in the document/tweet list\n",
    "\n",
    "print(\"Index results for the term 'researcher': {}\\n\".format(index['researcher']))\n",
    "print(\"First 10 Index results for the term 'research': \\n{}\".format(index['research'][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
